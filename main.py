import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import csv
import time
from datetime import datetime
import logging
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
import re
from dataclasses import dataclass
from pathlib import Path
import sqlite3
from fake_useragent import UserAgent
from datetime import timedelta

# ConfiguraÃ§Ã£o de logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapingConfig:
    """ConfiguraÃ§Ã£o para scraping"""
    delay: float = 1.0  # Delay entre requests
    timeout: int = 10
    max_retries: int = 3
    use_random_agent: bool = True
    output_format: str = 'csv'  # csv, json, excel, sqlite
    output_path: str = 'scraped_data'

class WebScraper:
    """Classe base para web scraping"""
    
    def __init__(self, config: ScrapingConfig):
        self.config = config
        self.session = requests.Session()
        self.ua = UserAgent() if config.use_random_agent else None
        self.scraped_data = []
        
        # Headers padrÃ£o
        self.headers = {
            'User-Agent': self.get_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        }
        self.session.headers.update(self.headers)
    
    def get_user_agent(self) -> str:
        """Retorna user agent"""
        if self.ua:
            return self.ua.random
        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    
    def make_request(self, url: str, method: str = 'GET', **kwargs) -> Optional[requests.Response]:
        """Faz requisiÃ§Ã£o com retry e delay"""
        for attempt in range(self.config.max_retries):
            try:
                time.sleep(self.config.delay)
                
                response = self.session.request(
                    method=method,
                    url=url,
                    timeout=self.config.timeout,
                    **kwargs
                )
                
                if response.status_code == 200:
                    return response
                else:
                    logger.warning(f"Status {response.status_code} para {url}")
                    
            except Exception as e:
                logger.warning(f"Tentativa {attempt + 1} falhou para {url}: {str(e)}")
                
                if attempt < self.config.max_retries - 1:
                    time.sleep(self.config.delay * (attempt + 1))
        
        logger.error(f"Falha ao acessar {url} apÃ³s {self.config.max_retries} tentativas")
        return None
    
    def parse_html(self, html: str) -> BeautifulSoup:
        """Parseia HTML com BeautifulSoup"""
        return BeautifulSoup(html, 'html.parser')
    
    def save_data(self, data: List[Dict], filename: str):
        """Salva dados no formato especificado"""
        if not data:
            logger.warning("Nenhum dado para salvar")
            return
        
        # Criar diretÃ³rio se nÃ£o existir
        Path(self.config.output_path).mkdir(parents=True, exist_ok=True)
        
        filepath = Path(self.config.output_path) / filename
        
        try:
            if self.config.output_format == 'csv':
                df = pd.DataFrame(data)
                df.to_csv(f"{filepath}.csv", index=False, encoding='utf-8')
                
            elif self.config.output_format == 'json':
                with open(f"{filepath}.json", 'w', encoding='utf-8') as f:
                    json.dump(data, f, indent=2, ensure_ascii=False)
                    
            elif self.config.output_format == 'excel':
                df = pd.DataFrame(data)
                df.to_excel(f"{filepath}.xlsx", index=False)
                
            elif self.config.output_format == 'sqlite':
                self.save_to_sqlite(data, f"{filepath}.db")
            
            logger.info(f"Dados salvos em: {filepath}.{self.config.output_format}")
            
        except Exception as e:
            logger.error(f"Erro ao salvar dados: {str(e)}")
    
    def save_to_sqlite(self, data: List[Dict], db_path: str):
        """Salva dados em SQLite"""
        conn = sqlite3.connect(db_path)
        df = pd.DataFrame(data)
        df.to_sql('scraped_data', conn, if_exists='replace', index=False)
        conn.close()

class JobScraper(WebScraper):
    """Scraper para vagas de emprego"""
    
    def scrape_jobs(self, search_terms: List[str], location: str = "SÃ£o Paulo"):
        """Scrapa vagas de emprego"""
        jobs_data = []
        
        # Exemplo com dados simulados (substitua por sites reais)
        for term in search_terms:
            logger.info(f"Buscando vagas para: {term}")
            
            # SimulaÃ§Ã£o de dados de vagas
            sample_jobs = self.generate_sample_jobs(term, location)
            jobs_data.extend(sample_jobs)
        
        return jobs_data
    
    def generate_sample_jobs(self, term: str, location: str) -> List[Dict]:
        """Gera dados simulados de vagas"""
        import random
        
        companies = [
            "TechCorp", "InnovaSoft", "DataSolutions", "CloudTech", 
            "StartupXYZ", "MegaCorp", "DigitalLabs", "CodeFactory"
        ]
        
        levels = ["JÃºnior", "Pleno", "SÃªnior"]
        salaries = [3000, 4500, 6000, 8000, 10000, 12000]
        
        jobs = []
        for i in range(random.randint(5, 15)):
            job = {
                'title': f"Desenvolvedor {term} {random.choice(levels)}",
                'company': random.choice(companies),
                'location': location,
                'salary': f"R$ {random.choice(salaries):,}",
                'description': f"Vaga para {term} com experiÃªncia em desenvolvimento web",
                'requirements': f"Python, {term}, Git, SQL",
                'url': f"https://example.com/job/{i}",
                'posted_date': datetime.now().strftime('%Y-%m-%d'),
                'scraped_at': datetime.now().isoformat()
            }
            jobs.append(job)
        
        return jobs

class EcommerceScraper(WebScraper):
    """Scraper para dados de e-commerce"""
    
    def scrape_products(self, categories: List[str]) -> List[Dict]:
        """Scrapa produtos de e-commerce"""
        products_data = []
        
        for category in categories:
            logger.info(f"Buscando produtos da categoria: {category}")
            
            # SimulaÃ§Ã£o de dados de produtos
            sample_products = self.generate_sample_products(category)
            products_data.extend(sample_products)
        
        return products_data
    
    def generate_sample_products(self, category: str) -> List[Dict]:
        """Gera dados simulados de produtos"""
        import random
        
        base_products = {
            'eletrÃ´nicos': ['Smartphone', 'Tablet', 'Notebook', 'Fone', 'CÃ¢mera'],
            'roupas': ['Camiseta', 'CalÃ§a', 'Vestido', 'Casaco', 'TÃªnis'],
            'casa': ['Mesa', 'Cadeira', 'SofÃ¡', 'Cama', 'Geladeira'],
            'livros': ['Romance', 'TÃ©cnico', 'Biografia', 'FicÃ§Ã£o', 'HistÃ³ria']
        }
        
        products = base_products.get(category.lower(), ['Produto GenÃ©rico'])
        
        product_list = []
        for i in range(random.randint(8, 20)):
            product = {
                'name': f"{random.choice(products)} {random.randint(1, 100)}",
                'category': category,
                'price': round(random.uniform(50, 2000), 2),
                'rating': round(random.uniform(3.0, 5.0), 1),
                'reviews_count': random.randint(10, 500),
                'availability': random.choice(['Em estoque', 'Ãšltimas unidades', 'IndisponÃ­vel']),
                'brand': f"Marca {random.choice(['A', 'B', 'C', 'D'])}",
                'url': f"https://example.com/product/{i}",
                'scraped_at': datetime.now().isoformat()
            }
            product_list.append(product)
        
        return product_list

class NewsScraper(WebScraper):
    """Scraper para notÃ­cias"""
    
    def scrape_news(self, topics: List[str]) -> List[Dict]:
        """Scrapa notÃ­cias"""
        news_data = []
        
        for topic in topics:
            logger.info(f"Buscando notÃ­cias sobre: {topic}")
            
            # SimulaÃ§Ã£o de dados de notÃ­cias
            sample_news = self.generate_sample_news(topic)
            news_data.extend(sample_news)
        
        return news_data
    
    def generate_sample_news(self, topic: str) -> List[Dict]:
        """Gera dados simulados de notÃ­cias"""
        import random
        
        sources = ["TechNews", "InfoDaily", "TechCrunch Brasil", "StartupBR", "DevNews"]
        
        news_list = []
        for i in range(random.randint(5, 12)):
            news = {
                'title': f"Ãšltimas novidades sobre {topic} - {random.randint(1, 100)}",
                'source': random.choice(sources),
                'author': f"Autor {random.randint(1, 10)}",
                'published_date': (datetime.now() - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d'),
                'topic': topic,
                'summary': f"Resumo da notÃ­cia sobre {topic}...",
                'url': f"https://example.com/news/{i}",
                'scraped_at': datetime.now().isoformat()
            }
            news_list.append(news)
        
        return news_list

class DataAnalyzer:
    """Classe para anÃ¡lise dos dados coletados"""
    
    def __init__(self, data: List[Dict]):
        self.data = data
        self.df = pd.DataFrame(data) if data else pd.DataFrame()
    
    def get_basic_stats(self) -> Dict:
        """Retorna estatÃ­sticas bÃ¡sicas"""
        if self.df.empty:
            return {}
        
        stats = {
            'total_records': len(self.df),
            'columns': list(self.df.columns),
            'data_types': self.df.dtypes.to_dict(),
            'missing_values': self.df.isnull().sum().to_dict(),
            'unique_values': {col: self.df[col].nunique() for col in self.df.columns}
        }
        
        return stats
    
    def analyze_text_data(self, text_column: str) -> Dict:
        """Analisa dados de texto"""
        if text_column not in self.df.columns:
            return {}
        
        text_data = self.df[text_column].dropna()
        
        analysis = {
            'total_entries': len(text_data),
            'average_length': text_data.str.len().mean(),
            'max_length': text_data.str.len().max(),
            'min_length': text_data.str.len().min(),
            'most_common_words': self.get_common_words(text_data.str.cat(sep=' '))
        }
        
        return analysis
    
    def get_common_words(self, text: str, top_n: int = 10) -> List[tuple]:
        """Retorna palavras mais comuns"""
        # Limpar texto
        words = re.findall(r'\b\w+\b', text.lower())
        
        # Remover stop words bÃ¡sicas
        stop_words = {'de', 'da', 'do', 'com', 'para', 'em', 'e', 'o', 'a', 'os', 'as', 'um', 'uma'}
        words = [word for word in words if word not in stop_words and len(word) > 2]
        
        # Contar frequÃªncia
        word_count = {}
        for word in words:
            word_count[word] = word_count.get(word, 0) + 1
        
        # Retornar top N
        return sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:top_n]

def main():
    """FunÃ§Ã£o principal para demonstraÃ§Ã£o"""
    print("ğŸ•·ï¸  Web Scraping Tools - DemonstraÃ§Ã£o")
    print("=" * 50)
    
    # ConfiguraÃ§Ã£o
    config = ScrapingConfig(
        delay=1.0,
        output_format='csv',
        output_path='scraped_data'
    )
    
    # 1. Scraping de Vagas de Emprego
    print("\nğŸ’¼ Scraped Jobs...")
    job_scraper = JobScraper(config)
    jobs_data = job_scraper.scrape_jobs(['Python', 'Django', 'JavaScript'], 'SÃ£o Paulo')
    job_scraper.save_data(jobs_data, 'jobs_data')
    
    # 2. Scraping de E-commerce
    print("\nğŸ›’ Scraping E-commerce...")
    ecommerce_scraper = EcommerceScraper(config)
    products_data = ecommerce_scraper.scrape_products(['eletrÃ´nicos', 'roupas', 'casa'])
    ecommerce_scraper.save_data(products_data, 'products_data')
    
    # 3. Scraping de NotÃ­cias
    print("\nğŸ“° Scraping NotÃ­cias...")
    news_scraper = NewsScraper(config)
    news_data = news_scraper.scrape_news(['Python', 'IA', 'Tecnologia'])
    news_scraper.save_data(news_data, 'news_data')
    
    # 4. AnÃ¡lise dos Dados
    print("\nğŸ“Š AnÃ¡lise dos Dados...")
    
    # Analisar dados de jobs
    job_analyzer = DataAnalyzer(jobs_data)
    job_stats = job_analyzer.get_basic_stats()
    
    print(f"âœ… Jobs coletados: {job_stats.get('total_records', 0)}")
    print(f"âœ… Produtos coletados: {len(products_data)}")
    print(f"âœ… NotÃ­cias coletadas: {len(news_data)}")
    
    # AnÃ¡lise de texto dos tÃ­tulos de jobs
    if jobs_data:
        text_analysis = job_analyzer.analyze_text_data('title')
        print(f"ğŸ“ AnÃ¡lise de tÃ­tulos de vagas:")
        print(f"   - MÃ©dia de caracteres: {text_analysis.get('average_length', 0):.1f}")
        print(f"   - Palavras mais comuns: {text_analysis.get('most_common_words', [])[:5]}")
    
    # Resumo final
    print("\nğŸ‰ Scraping ConcluÃ­do!")
    print(f"ğŸ“ Dados salvos em: {config.output_path}")
    print(f"ğŸ“Š Total de registros: {len(jobs_data) + len(products_data) + len(news_data)}")
    
    # Exemplo de uso avanÃ§ado
    print("\nğŸ”§ Exemplo de Uso AvanÃ§ado:")
    print("job_scraper = JobScraper(config)")
    print("data = job_scraper.scrape_jobs(['Python Junior'], 'SÃ£o Paulo')")
    print("job_scraper.save_data(data, 'python_jobs')")

if __name__ == "__main__":
    main()

